{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6章　テキストの取り扱い \n",
    "## レシピ6.1　テキストのクリーニング \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを生成\n",
    "text_data = [\"   Interrobang. By Aishwarya Henriette     \",\n",
    "             \"Parking And Going. By Karl Gautier\",\n",
    "             \"    Today Is The night. By Jarek Prakash   \"]\n",
    "\n",
    "# ホワイトスペースを削除\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "# テキストを表示\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ピリオドを削除\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "# テキストを表示\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数を定義\n",
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "# 関数を適用\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import re\n",
    "\n",
    "# 関数を定義\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "# 関数を適用\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字列を定義\n",
    "s = \"machine learning in python cookbook\"\n",
    "\n",
    "# 文字\"n\"の最初のインデックスを見つける\n",
    "find_n = s.find(\"n\")\n",
    "\n",
    "# 文字列の先頭が\"m\"かどうか\n",
    "starts_with_m = s.startswith(\"m\")\n",
    "\n",
    "# 文字列の末尾が\"python\"かどうか\n",
    "ends_with_python = s.endswith(\"python\")\n",
    "\n",
    "# 文字列がアルファベットと数字のみで構成されているか\n",
    "is_alnum = s.isalnum()\n",
    "\n",
    "# アルファベットのみ（スペースを含まず）で構成されているか\n",
    "is_alpha = s.isalpha()\n",
    "\n",
    "# utf-8でエンコード\n",
    "encode_as_utf8 = s.encode(\"utf-8\")\n",
    "\n",
    "# utf-8でエンコードしたものをデコード\n",
    "decode = encode_as_utf8.decode(\"utf-8\")\n",
    "print(\n",
    "    find_n,\n",
    "    starts_with_m,\n",
    "    ends_with_python,\n",
    "    is_alnum,\n",
    "    is_alpha,\n",
    "    encode_as_utf8,\n",
    "    decode,\n",
    "    sep = \"|\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.2　HTMLのパースとクリーニング \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTMLテキストを作成\n",
    "html = \"<div class='full_name'>\"\\\n",
    "       \"<span style='font-weight:bold'>Masego\"\\\n",
    "       \"</span> Azra</div>\"\n",
    "\n",
    "# HTMLをパース\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# classが\"full_name\"となっているdivを見つけて、そのテキストを表示\n",
    "soup.find(\"div\", { \"class\" : \"full_name\" }).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.3　句読点の除去 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# テキストを作成\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "             '10000% Agree!!!! #LoveIT',\n",
    "             'Right?!?!']\n",
    "\n",
    "# 句読点文字を含む辞書を作成\n",
    "punctuation = dict.fromkeys(\n",
    "    (i for i in range(sys.maxunicode)\n",
    "        if unicodedata.category(chr(i)).startswith('P')\n",
    "    ),\n",
    "    None\n",
    ")\n",
    "\n",
    "# それぞれの文字列から、句読点文字をすべて除去\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.4　テキストのトークン化 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 最初の1回は下のコメント外してリソースをダウンロード\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# テキストを生成\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# 単語単位でトークン化\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# テキストを生成\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "# 文章単位でトークン化\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.5　ストップワードの除去 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 最初の1回は下のコメントを外して、ストップワードをダウンロード\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# 単語トークン列を作成\n",
    "tokenized_words = ['i',\n",
    "                   'am',\n",
    "                   'going',\n",
    "                   'to',\n",
    "                   'go',\n",
    "                   'to',\n",
    "                   'the',\n",
    "                   'store',\n",
    "                   'and',\n",
    "                   'park']\n",
    "\n",
    "# ストップワードをロード\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# ストップワードを削除\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワードを表示\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.6　語幹の抽出 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# 単語トークンを作成\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "\n",
    "# 語幹抽出器を作成\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# 語幹抽出器を適用\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.7　品詞タグ付け \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# 最初の1回は下のコメント外してリソースをダウンロード\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# テキストを生成\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# 訓練済み品詞タグ付け器を適用\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# 品詞を表示\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 品詞を用いて単語を選択\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# テキストを生成\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "          \"Political science is an amazing field\",\n",
    "          \"San Francisco is an awesome city\"]\n",
    "\n",
    "# リストを作成\n",
    "tagged_tweets = []\n",
    "\n",
    "# ツイート中の単語にタグ付け\n",
    "for tweet in tweets:\n",
    "    tweet_tag = pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "# ワンホットエンコードを用いて、タグを特徴量に変換\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量名を表示\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.8　固有表現抽出を行う \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import spacy\n",
    "\n",
    "# spaCyのパッケージをロードし、テキストを分析する\n",
    "# 事前に\"python -m spacy download en\"を実行しておくこと\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money.\")\n",
    "\n",
    "# 固有表現を表示\n",
    "print(doc.ents)\n",
    "\n",
    "# 個々の固有表現に対してテキストと固有表現ラベルを表示\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.9　BoW（Bag of Words）によるテキストエンコード \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# テキストを生成\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# BoW特徴量行列を作成\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# 特徴量行列を表示\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量名を表示\n",
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータを指定して特徴量行列を作成\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                              stop_words=\"english\",\n",
    "                              vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "\n",
    "# 特徴量行列を表示\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gramと2-gramを表示\n",
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.10　単語への重み付け \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# テキストを生成\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# tf-idf特徴量行列を作成\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# tf-idf特徴量行列を表示\n",
    "feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf特徴量行列を密行列として表示\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量名を表示\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.11　 テキスト検索でテキストベクトルを用いてテキストの類似性を計算する \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# 検索対象テキストデータを作成\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# tf-idf特徴量行列を作成\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# 検索クエリを作成し、tf-idfベクトルに変換\n",
    "text = \"Brazil is the best\"\n",
    "vector = tfidf.transform([text])\n",
    "\n",
    "# 入力のベクトルと検索対象すべてのベクトルとのコサイン類似度を計算\n",
    "cosine_similarities = linear_kernel(vector, feature_matrix).flatten()\n",
    "\n",
    "# 関連度が高い順にインデックスをソート\n",
    "related_doc_indicies = cosine_similarities.argsort()[:-10:-1]\n",
    "\n",
    "# 検索文字列との類似度が高い順に、テキストとコサイン類似度を表示\n",
    "print([(text_data[i], cosine_similarities[i]) for i in related_doc_indicies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ6.12　感情分析クラス分類器を使用する \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from transformers import pipeline\n",
    "\n",
    "# 感情分析を行うNLPパイプラインを作成\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# テキストの感情分析クラス分類を行う\n",
    "# （最初に実行するとデータとモデルをダウンロードする）\n",
    "sentiment_1 = classifier(\"I hate machine learning! It's the absolute worst.\")\n",
    "sentiment_2 = classifier(\n",
    "    \"Machine learning is the absolute\"\n",
    "    \"bees knees I love it so much!\"\n",
    ")\n",
    "\n",
    "# 感情を表示\n",
    "print(sentiment_1, sentiment_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
