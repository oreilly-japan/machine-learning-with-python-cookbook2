{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22章　非構造化データ向けのニューラルネットワーク \n",
    "## レシピ22.1　画像クラス分類を行うニューラルネットワークの訓練 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# コンボリューショナルニューラルネットワークのアーキテクチャを定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(self.dropout1(x), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(self.dropout2(x)))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# 実行するデバイスを設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# データの前処理を定義\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# MNISTデータセットをロード\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "    transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# データローダを作成\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "# モデルと最適化器を初期化\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# torch 2.0の最適化器を用いてモデルをコンパイル\n",
    "model = torch.compile(model)\n",
    "\n",
    "# 訓練ループを定義\n",
    "model.train()\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = nn.functional.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# テストループを定義\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        \n",
    "        # 確率の対数が最大となるインデックスを取得\n",
    "        test_loss += nn.functional.nll_loss(\n",
    "            output, target, reduction='sum'\n",
    "        ).item() # バッチロスに加算\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "test_loss /= len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ22.2　 テキストのクラス分類を行うニューラルネットワークの訓練 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 20 newsgroupデータセットをロード\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', shuffle=True,\n",
    "    random_state=42, categories=cats)\n",
    "\n",
    "# データセットを訓練セットとテストセットに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups_data.data,\n",
    "    newsgroups_data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bag-of-words手法を用いてテキストデータをベクトル化\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# モデルを定義\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# モデルを作成し、ロス関数と最適化器を定義\n",
    "model = TextClassifier(num_classes=len(cats))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# torch 2.0の最適化器を用いてモデルをコンパイル\n",
    "model = torch.compile(model)\n",
    "\n",
    "# モデルを訓練\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "num_batches = len(X_train) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        # 対象のバッチの入力とターゲットを用意\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        inputs = X_train[start_idx:end_idx]\n",
    "        targets = y_train[start_idx:end_idx]\n",
    "\n",
    "        # 最適化器の保持する勾配をゼロにリセット\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # モデルにデータを順伝搬して損失を計算\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "    \n",
    "        # 逆伝搬してパラメータを更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # エポック全体のロスを更新\n",
    "        total_loss += loss.item()\n",
    "\n",
    "# このエポックでのテストセットに対する精度を計算\n",
    "test_outputs = model(X_test)\n",
    "test_predictions = torch.argmax(test_outputs, dim=1)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "# エポック番号、平均ロス、テスト精度を出力\n",
    "print(f\"Epoch: {epoch+1}, Loss: {total_loss/num_batches}, Test Accuracy:\"\n",
    "    f\"{test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ22.3　 訓練済みモデルのファインチューニングによる画像クラス分類 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "import torch\n",
    "from torchvision.transforms import(\n",
    "    RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "    )\n",
    "from transformers import Trainer, TrainingArguments, DefaultDataCollator\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from datasets import load_dataset, load_metric, Image\n",
    "\n",
    "# 画像をRGBに変換するヘルパ関数を定義\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in\n",
    "    examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# メトリクスを計算するヘルパ関数を定義\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1),\n",
    "        references=p.label_ids)\n",
    "\n",
    "# fashion mnistデータセットをロード\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "\n",
    "# VITモデルから画像プロセッサをロード\n",
    "image_processor = ViTFeatureExtractor.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "\n",
    "# データセットから取り出したラベルを設定\n",
    "labels = dataset['train'].features['label'].names\n",
    "\n",
    "# 訓練済みモデルをロード\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "\n",
    "# コレータ、ノーマライザ、画像変換をロード。\n",
    "collate_fn = DefaultDataCollator()\n",
    "normalize = Normalize(mean=image_processor.image_mean,\n",
    "    std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "\n",
    "# 変換に用いるデータセットをロード\n",
    "dataset = dataset.with_transform(transforms)\n",
    "\n",
    "# メトリクスとしてaccuracyを使用\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# 訓練時の引数を設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fashion_mnist_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Trainerを作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "# モデルを訓練し、メトリクスを記録し、セーブする\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レシピ22.4　 訓練済みモデルのファインチューニングによるテキストクラス分類 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをロード\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    )\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# imdbデータセットをロード\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# トークナイザとコレータを作成\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# imdbデータセットをトークナイズ\n",
    "tokenized_imdb = imdb.map(\n",
    "    lambda example: tokenizer(\n",
    "        example[\"text\"], padding=\"max_length\", truncation=True\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# accuracyをメトリクスとして使用\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# メトリクスを計算するヘルパ関数を定義\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# インデックスとラベル間のマッピングを行う辞書を作成\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "# 訓練済みモデルをロード\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# 訓練引数を指定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainerを作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# モデルを訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
